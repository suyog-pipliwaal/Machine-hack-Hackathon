{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0cf017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "root_dir = \"./Participants_Data_TWC/\"\n",
    "csv_file = root_dir+'train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcaa3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621f6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterMarkingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, root, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_file)\n",
    "        self.root_dir = root\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        img_name = os.path.join(self.root_dir+self.dataset.iloc[index, 0])\n",
    "        img = io.imread(img_name)\n",
    "        label = self.dataset.iloc[index, 1]\n",
    "        label = np.array([label])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return {\"image\":img,\"label\":label}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea8426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform=transforms.Compose(\n",
    "#         [transforms.Resize(32),transforms.ToTensor()]\n",
    "#     ),\n",
    "watermark_dataset = WaterMarkingDataset(\n",
    "    \"./Participants_Data_TWC/train.csv\", \n",
    "    './Participants_Data_TWC/train/',\n",
    "    transform=transforms.Compose([transforms.ToPILImage(), transforms.Resize(256),transforms.ToTensor() ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658d658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# for i in range(len(watermark_dataset)):\n",
    "#     sample = watermark_dataset[i]\n",
    "#     print(i, sample['image'].shape, sample['label'].shape)\n",
    "\n",
    "#     ax = plt.subplot(1, 4, i + 1)\n",
    "#     plt.tight_layout()\n",
    "#     ax.set_title('Sample #{}'.format(i))\n",
    "#     ax.axis('off')\n",
    "#     plt.imshow(sample['image'])\n",
    "\n",
    "#     if i == 3:\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63652f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "validation_split = .2\n",
    "shuffle_dataset= True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(watermark_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(watermark_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(watermark_dataset, batch_size=batch_size,sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8f0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32*126*126, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = torch.permute(x, (0,3,2,1))\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(nn.functional.relu(self.conv2(x)), 2)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 32*126*126)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d37254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(epoch_label, model, optimizer, calculate_loss):\n",
    "    save_filename = 'net_%s.pth' % epoch_label\n",
    "    save_filename = 'net_%s.pth' % epoch_label\n",
    "    save_path = os.path.join('./phase_4', save_filename)\n",
    "    torch.save({'epoch': epoch_label,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': calculate_loss,}, save_path)\n",
    "\n",
    "    \n",
    "def fit(train_loader, model, loss_function, optimizer, device):\n",
    "    model=model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=\"Training\",bar_format='{desc:<5.5}{percentage:4.0f}%|{bar:10}{r_bar}') ):\n",
    "        optimizer.zero_grad()\n",
    "        images = batch['image'].to(device)\n",
    "        target = batch['label'].to(device)\n",
    "        \n",
    "        output = model(images.float())\n",
    "        loss = loss_function(output, target.float())\n",
    "                                   \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        predicted = (output > 0.5).float()\n",
    "        total += target.size(0)\n",
    "#         print(\"====>\", predicted,target)\n",
    "        correct += (predicted == target).sum().item()\n",
    "#         print(\"correct, total :\", correct, total)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3eb6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_loader, model, loss_function, optimizer, device):\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(tqdm(val_loader, desc=\"validating\", bar_format='{desc:<5.5}{percentage:4.0f}%|{bar:10}{r_bar}')):\n",
    "        images = batch['image'].to(device)\n",
    "        target = batch['label'].to(device)\n",
    "        \n",
    "        output = model(images.float())\n",
    "        loss = loss_function(output, target.float())\n",
    "        \n",
    "        val_loss += loss.data.item()*images.size(0)\n",
    "        predicted = (output > 0.5).float()\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct / total\n",
    "    return val_loss, val_accuracy\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c34767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,val_loader, model, loss_function, optimizer, epochs, device):\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "            optimizer.param_groups[0]['lr'], epochs, device)\n",
    "        )\n",
    "    start_time = time.time()\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "    history['val_acc'] = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = fit(train_loader, model, loss_function, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(val_loader, model, loss_function, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        save_network(epoch_label=epoch, model=model, optimizer=optimizer, calculate_loss=loss_function)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {train_loss:.4f}, Training Acc: {train_acc:.4f}, Val Loss:{val_loss:.4f}, Val Acc:{val_acc:.4f}\")\n",
    "    total_time_sec     =  time.time() - start_time\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f3fdf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=BinaryImageClassifier, opt=Adam(lr=0.001000), epochs=25, device=cpu\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [13:02<00:00,  3.91s/it]\n",
      "valid 100%|██████████| 50/50 [01:08<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Training Loss: 2766.7169, Training Acc: 0.5630, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [12:37<00:00,  3.79s/it]\n",
      "valid 100%|██████████| 50/50 [01:04<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [12:28<00:00,  3.74s/it]\n",
      "valid 100%|██████████| 50/50 [01:08<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [12:38<00:00,  3.79s/it]\n",
      "valid 100%|██████████| 50/50 [01:08<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [12:45<00:00,  3.83s/it]\n",
      "valid 100%|██████████| 50/50 [01:04<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [13:18<00:00,  3.99s/it]\n",
      "valid 100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 100%|██████████| 200/200 [14:52<00:00,  4.46s/it]\n",
      "valid 100%|██████████| 50/50 [01:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Training Loss: 2779.5000, Training Acc: 0.5639, Val Loss:2826.0000, Val Acc:0.5565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train   6%|▋         | 13/200 [00:55<13:25,  4.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      5\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m\n\u001b[0;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, loss_function, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 13\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(val_loader, model, loss_function, optimizer, device)\n\u001b[1;32m     15\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_loader, model, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{desc:<5.5}\u001b[39;00m\u001b[38;5;132;01m{percentage:4.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;132;01m{bar:10}\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) ):\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mWaterMarkingDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([label])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 16\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:img,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:label}\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torchvision/transforms/transforms.py:60\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 60\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torchvision/transforms/transforms.py:195\u001b[0m, in \u001b[0;36mResize.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m        img (PIL Image): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m        PIL Image: Rescaled image.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/torchvision/transforms/functional.py:244\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    242\u001b[0m         oh \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    243\u001b[0m         ow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(size \u001b[38;5;241m*\u001b[39m w \u001b[38;5;241m/\u001b[39m h)\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], interpolation)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyt_m1/lib/python3.9/site-packages/PIL/Image.py:2008\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2001\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2002\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2003\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2004\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2005\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2006\u001b[0m         )\n\u001b[0;32m-> 2008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "model = BinaryImageClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=25\n",
    "history = train(train_loader, validation_loader, model, criterion, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ac3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae95378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
